@article{prance2025,
  title = {PRANCE: Joint Token-Optimization and Structural Channel-Pruning for Adaptive ViT Inference},
  author = {},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI)},
  year = {2025},
  pdf = {https://arxiv.org/abs/2407.05010},
  code = {https://github.com/ChildTang/PRANCE},
  preview = {prance.png},
  description = {联合 token 优化与结构剪枝；样本级动态决策 token 数量与通道配置；Result-to-Go 让奖励更稠密、收敛更稳。ImageNet 上仅保留约 10\% token 仍保持无损 Top-1，FLOPs 平均减少超过一倍。},
  keywords = {Vision Transformer, Token Pruning, Channel Pruning, Efficient Inference}
}

@inproceedings{qdit2025,
  title = {Q-DiT: Accurate Post-Training Quantization for Diffusion Transformers},
  author = {},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2025},
  pdf = {https://arxiv.org/pdf/2406.17343},
  code = {https://github.com/Juanerx/Q-DiT},
  preview = {q-dit.png},
  description = {解决 DiT 量化中\"通道/维度方差大\"和\"激活随扩散步数动态变化\"两大难点；提出自动粒度分配 + 动态量化。W6A8 下无损量化；相对现有 SOTA，FID 下降约 20\%。},
  keywords = {Diffusion Transformer, Post-Training Quantization, Efficient Inference}
}

@inproceedings{joint2025aaai,
  title = {Joint Automatic Architecture Design and Low-Bit Quantization with Hardware-Software Co-Exploration},
  author = {},
  booktitle = {AAAI Conference on Artificial Intelligence (AAAI)},
  year = {2025},
  pdf = {https://arxiv.org/pdf/2501.05339},
  preview = {aaai2025.png},
  description = {联合优化网络结构、超低混合精度位宽与加速器架构；通道级稀疏量化缓解显存，硬件生成网络加速搜索。},
  keywords = {Neural Architecture Search, Mixed Precision, Hardware-Software Co-Design}
}

@inproceedings{rfquant2024,
  title = {Retraining-free Model Quantization via One-Shot Weight-Coupling Learning},
  author = {},
  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2401.01543.pdf},
  code = {https://github.com/1hunters/retraining-free-quantization},
  preview = {rf-quant.png},
  description = {分析位宽干扰，提出位宽调度器 + 对齐策略提升稳定性；无需重训练，性能超过已有方法。},
  keywords = {Quantization, One-Shot, Efficient Inference}
}

@inproceedings{advrobust2024,
  title = {Investigating the Impact of Quantization on Adversarial Robustness},
  author = {},
  booktitle = {ICLR PML4LRS Workshop},
  year = {2024},
  pdf = {https://arxiv.org/pdf/2404.05639.pdf},
  preview = {adv-robust.png},
  description = {定义量化流水线并拆解组件，明确不同量化组件对对抗防御的影响。},
  keywords = {Quantization, Adversarial Robustness}
}

@inproceedings{elasticvit2023,
  title = {ElasticViT: Conflict-aware Supernet Training for Deploying Fast Vision Transformer on Diverse Mobile Devices},
  author = {},
  booktitle = {IEEE/CVF International Conference on Computer Vision (ICCV)},
  year = {2023},
  pdf = {https://arxiv.org/pdf/2303.09730.pdf},
  code = {https://github.com/microsoft/Moonlit/tree/main/ElasticViT},
  preview = {elasticvit.png},
  description = {端上推理速度最高提升 2x。},
  keywords = {Vision Transformer, Mobile Deployment, Efficient Inference}
}

@inproceedings{abn2022,
  title = {Arbitrary Bit-width Network: A Joint Layer-Wise Quantization and Adaptive Inference Approach},
  author = {},
  booktitle = {ACM International Conference on Multimedia (ACM MM)},
  year = {2022},
  pdf = {https://arxiv.org/pdf/2204.09992.pdf},
  preview = {abn.png},
  description = {相较高度压缩模型，最高节省 10\%-15\% 计算资源。},
  keywords = {Quantization, Adaptive Inference}
}

@inproceedings{limpq2022,
  title = {Mixed-Precision Neural Network Quantization via Learned Layer-Wise Importance},
  author = {},
  booktitle = {European Conference on Computer Vision (ECCV)},
  year = {2022},
  pdf = {https://arxiv.org/pdf/2203.08368.pdf},
  code = {https://github.com/1hunters/LIMPQ},
  preview = {limpq.png},
  description = {位宽策略搜索效率最高提速约 300x。},
  keywords = {Mixed Precision, Quantization}
}

@inproceedings{seam2023,
  title = {SEAM: Searching Transferable Mixed-Precision Quantization Policy through Large Margin Regularization},
  author = {},
  booktitle = {ACM International Conference on Multimedia (ACM MM)},
  year = {2023},
  pdf = {https://arxiv.org/pdf/2302.06845.pdf},
  preview = {seam.png},
  description = {通过大间隔正则搜索可迁移混合精度量化策略。},
  keywords = {Mixed Precision, Quantization Policy}
}

@misc{tmpqdm2024,
  title = {TMPQ-DM: Joint Timestep Reduction and Quantization Precision Selection for Efficient Diffusion Models},
  author = {},
  year = {2024},
  journal = {arXiv preprint},
  pdf = {https://arxiv.org/pdf/2404.09532.pdf},
  preview = {tmpqdm.png},
  description = {5 个数据集上实现 >10x BitOPs 节省，同时保持相同生成性能。},
  keywords = {Diffusion Model, Timestep Reduction, Quantization}
}
